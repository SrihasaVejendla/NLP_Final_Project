{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmphasisSelection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q-3MHNGoKuz",
        "outputId": "ac2f258a-018c-45c7-8882-7d1a38fe4b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov  3 01:19:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSLKZ2lcJx06",
        "outputId": "08cf9f99-48a7-4439-d663-42dd6d5b06bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6KNiijNYN-K"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpetzFRcWBmg"
      },
      "source": [
        "def dataset(filename):\n",
        "    with open(filename,'r') as fp:\n",
        "        lines = [line.strip() for line in fp]\n",
        "    return lines"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwPuzPYsWBp3"
      },
      "source": [
        "def wordData(data):\n",
        "    wordLines = data\n",
        "    words = []\n",
        "    probabilities = []\n",
        "    wordList = []\n",
        "    pos = []\n",
        "    empty = []\n",
        "    for line in wordLines:\n",
        "        lineSplit = line.strip().split('\\t')\n",
        "        if line:\n",
        "            word = lineSplit[1]\n",
        "            prob = lineSplit[4]\n",
        "            temp = lineSplit[5]\n",
        "            words.append(word)\n",
        "            probabilities.append(prob)\n",
        "            pos.append(temp)\n",
        "        elif not (len(empty) and []):\n",
        "            wordList.append((words, pos, probabilities))\n",
        "            words = []\n",
        "            probabilities = []\n",
        "            pos = []\n",
        "    return wordList"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ugzYtznWB1X"
      },
      "source": [
        "def wordtest(data):\n",
        "    wordLines = data\n",
        "    words = []\n",
        "    testWord = []\n",
        "    empty = []\n",
        "    for line in wordLines:\n",
        "        lineSplit = line.strip().split('\\t')\n",
        "        if line:\n",
        "            word = lineSplit[1]            \n",
        "            words.append(word)\n",
        "        elif not len(empty):\n",
        "            testWord.append(words)\n",
        "            words = []       \n",
        "    return testWord"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpkAMX2uWB3y"
      },
      "source": [
        "def preTokenizing(data):\n",
        "    t = []\n",
        "    po = []\n",
        "    prob = []\n",
        "    for i,j,k in data:\n",
        "        text = []\n",
        "        pos = []\n",
        "        probs = []\n",
        "        for l in i:\n",
        "            text.append(l)\n",
        "        for m in j:\n",
        "            pos.append(m)\n",
        "        for n in k:\n",
        "            probs.append(float(n))\n",
        "        t.append(text)\n",
        "        po.append(pos)\n",
        "        prob.append(probs)\n",
        "    return t,po, prob"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n7FCyJQWB7K"
      },
      "source": [
        "TRAINING_FILE = \"train.txt\"\n",
        "DEV_FILE = \"dev.txt\"\n",
        "TEST_FILE = \"test_data.txt\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpfUuPwYHgf"
      },
      "source": [
        "trainText = wordData(dataset(TRAINING_FILE))\n",
        "testEval = wordtest(dataset(TEST_FILE))\n",
        "devText = wordData(dataset(DEV_FILE))\n",
        "\n",
        "\n",
        "trainWords,trainTags, trainLabels = preTokenizing(trainText)\n",
        "devWords, devTags, devLabels = preTokenizing(devText)\n",
        "\n",
        "tokenized_text = []\n",
        "for i in trainWords:\n",
        "    sent = \"\"\n",
        "    for j in i:\n",
        "        if sent == \"\":\n",
        "            sent += j\n",
        "        else:\n",
        "            sent = sent + \" \" + j\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    tokenized_text.append(tokens)\n",
        "\n",
        "    \n",
        "\n",
        "# Encoding each word of the sentence and appending to a list\n",
        "en=[]\n",
        "for i in tokenized_text:\n",
        "  en1 = []\n",
        "  for j in i:\n",
        "    token_ids = en1.append(tokenizer.encode_plus(j, add_special_tokens=False, return_attention_mask=False, return_tensors='pt'))\n",
        "  en.append(en1)\n",
        "  en1 = []\n",
        "  \n",
        "# Getting the input id's only for the list and passing them to the model \n",
        "words = []  \n",
        "for d in en:\n",
        "    words2 = []\n",
        "    for i in d:\n",
        "        words2.append(i['input_ids'])\n",
        "    words.append(words2)\n",
        "    words2 = []     "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa8Kf3q2rgdx"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')\n",
        "        self.linear = nn.Linear(1024, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        _, pooled_output = self.bert(tokens)\n",
        "        linear_output = self.linear(pooled_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        proba = proba.data.tolist()\n",
        "        return proba"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B70NVVPoepQP",
        "outputId": "25c3203c-785a-438e-e6a6-0f591c364b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "optimizer = optim.Adamax(bert_clf.parameters(), lr=0.0001)\n",
        "bert_clf.train()\n",
        "token_answers = []\n",
        "for epoch_num in range(10):\n",
        "    print(\"epoch_num ---->{}\".format(epoch_num))\n",
        "    count = 0\n",
        "    for batch_data, batch_probs in zip(words, trainLabels):\n",
        "        bert_clf.zero_grad()\n",
        "        count = count + 1\n",
        "        answers_temp = []\n",
        "        for i in batch_data:\n",
        "            probas = bert_clf(i)\n",
        "            for item in probas:\n",
        "                for i in item:\n",
        "                    answers_temp.append(i)            \n",
        "        token_answers.append(answers_temp)\n",
        "        print(answers_temp)\n",
        "        answers_temp = []\n",
        "        if count==5:\n",
        "          break\n",
        "        try:\n",
        "          loss_func = nn.MSELoss(reduction = 'sum')\n",
        "          print(answers_temp.size, batch_probs.size)\n",
        "          batch_loss = loss_func(torch.tensor(answers_temp), torch.tensor(batch_probs))\n",
        "          batch_loss.backward()\n",
        "          optimizer.step()\n",
        "        except:\n",
        "          continue\n",
        "    \n",
        "      \n",
        "      "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch_num ---->0\n",
            "[0.5097906589508057, 0.4866931438446045, 0.4128294289112091]\n",
            "[0.5072095394134521, 0.4846435487270355, 0.45699164271354675, 0.5161570310592651, 0.44299373030662537, 0.44648268818855286, 0.4380336403846741, 0.5067108273506165, 0.5168303847312927, 0.5077243447303772]\n",
            "[0.4802769720554352, 0.48637089133262634, 0.5304993391036987, 0.457534521818161, 0.4928857684135437, 0.4711836576461792, 0.43170252442359924, 0.4164516031742096, 0.4971231520175934]\n",
            "[0.47517073154449463, 0.45780807733535767]\n",
            "[0.5229343771934509, 0.4100739061832428, 0.3948374390602112, 0.4990491569042206, 0.50278639793396, 0.485446959733963, 0.452657550573349, 0.5241411328315735, 0.4933161437511444, 0.44707414507865906, 0.48711445927619934, 0.47073113918304443]\n",
            "epoch_num ---->1\n",
            "[0.40720757842063904, 0.4803933799266815, 0.48927053809165955]\n",
            "[0.4160015881061554, 0.4383767247200012, 0.5118116736412048, 0.4929916560649872, 0.47873154282569885, 0.5265690088272095, 0.5134424567222595, 0.43914029002189636, 0.5070618987083435, 0.3827095031738281]\n",
            "[0.4599204659461975, 0.5064298510551453, 0.3825662434101105, 0.5012016296386719, 0.4960453510284424, 0.5261465311050415, 0.36233702301979065, 0.4508778154850006, 0.5111425518989563]\n",
            "[0.5291119813919067, 0.4697566032409668]\n",
            "[0.4125392735004425, 0.5045470595359802, 0.5022750496864319, 0.4457065761089325, 0.47997188568115234, 0.5519307851791382, 0.4837208688259125, 0.4367089569568634, 0.43357527256011963, 0.4661145806312561, 0.4491499364376068, 0.5215558409690857]\n",
            "epoch_num ---->2\n",
            "[0.43425309658050537, 0.3741942048072815, 0.48023363947868347]\n",
            "[0.5053002834320068, 0.3785076439380646, 0.4413154125213623, 0.4111189842224121, 0.3680229187011719, 0.46556612849235535, 0.5504308938980103, 0.44805750250816345, 0.4027174711227417, 0.5047470927238464]\n",
            "[0.4037476181983948, 0.45413345098495483, 0.46682390570640564, 0.3983216881752014, 0.5251700282096863, 0.41629961133003235, 0.4670916795730591, 0.5036576390266418, 0.45879289507865906]\n",
            "[0.4437407851219177, 0.5180646777153015]\n",
            "[0.4380377233028412, 0.5493542551994324, 0.38336965441703796, 0.4603134095668793, 0.5092737078666687, 0.49235740303993225, 0.43521055579185486, 0.5176584720611572, 0.4216132164001465, 0.3519563674926758, 0.48390161991119385, 0.4927177131175995]\n",
            "epoch_num ---->3\n",
            "[0.5171324610710144, 0.4082692265510559, 0.5105230808258057]\n",
            "[0.4463309943675995, 0.5414918065071106, 0.3558286130428314, 0.5290251970291138, 0.5034874677658081, 0.437948614358902, 0.4110729694366455, 0.46212854981422424, 0.4751680791378021, 0.4436979293823242]\n",
            "[0.5440155267715454, 0.42476558685302734, 0.4850899577140808, 0.4367178678512573, 0.430648535490036, 0.515001118183136, 0.5491422414779663, 0.4446529746055603, 0.5234942436218262]\n",
            "[0.42391860485076904, 0.4594126343727112]\n",
            "[0.5390070080757141, 0.4081498980522156, 0.5837287902832031, 0.4427407383918762, 0.5408151149749756, 0.47356894612312317, 0.5093389749526978, 0.4223009943962097, 0.541732907295227, 0.40126076340675354, 0.43233662843704224, 0.44415631890296936]\n",
            "epoch_num ---->4\n",
            "[0.42235326766967773, 0.4814668893814087, 0.49989548325538635]\n",
            "[0.5123059749603271, 0.4846813976764679, 0.5485288500785828, 0.39350831508636475, 0.4536950886249542, 0.4139260947704315, 0.3542019724845886, 0.45245686173439026, 0.45780348777770996, 0.49484527111053467]\n",
            "[0.4249643385410309, 0.45439204573631287, 0.4593919515609741, 0.47834259271621704, 0.4736257791519165, 0.4567842483520508, 0.4757513105869293, 0.43850189447402954, 0.4458003044128418]\n",
            "[0.481732040643692, 0.5297598838806152]\n",
            "[0.5139639377593994, 0.48774510622024536, 0.46304401755332947, 0.4930966794490814, 0.5458142161369324, 0.4355223476886749, 0.4471217095851898, 0.5626111626625061, 0.4702689051628113, 0.4719601273536682, 0.4279191195964813, 0.5027371048927307]\n",
            "epoch_num ---->5\n",
            "[0.4751371443271637, 0.503981351852417, 0.46833711862564087]\n",
            "[0.40396252274513245, 0.4442965090274811, 0.46800288558006287, 0.5084031820297241, 0.5451838970184326, 0.43155547976493835, 0.5522366762161255, 0.47707825899124146, 0.4125826060771942, 0.5144863128662109]\n",
            "[0.38910913467407227, 0.47239330410957336, 0.4727976620197296, 0.481678307056427, 0.47546693682670593, 0.4653652012348175, 0.5344053506851196, 0.44280341267585754, 0.49540185928344727]\n",
            "[0.5070220232009888, 0.3737119138240814]\n",
            "[0.4801657497882843, 0.4439496397972107, 0.46129176020622253, 0.5098935961723328, 0.4624837636947632, 0.4872235655784607, 0.462188720703125, 0.4428042769432068, 0.4504797160625458, 0.49655619263648987, 0.4917713403701782, 0.4741479754447937]\n",
            "epoch_num ---->6\n",
            "[0.5377278923988342, 0.44345158338546753, 0.49973633885383606]\n",
            "[0.4955165684223175, 0.384208083152771, 0.3951389193534851, 0.43903011083602905, 0.5164391994476318, 0.462100625038147, 0.43263566493988037, 0.37439456582069397, 0.4299318194389343, 0.5181367993354797]\n",
            "[0.43604204058647156, 0.4599597454071045, 0.41675567626953125, 0.5233277678489685, 0.5372644066810608, 0.49775344133377075, 0.5223013162612915, 0.49683982133865356, 0.4497407078742981]\n",
            "[0.5024548172950745, 0.5475705862045288]\n",
            "[0.47797301411628723, 0.5066083669662476, 0.4154317080974579, 0.4368273913860321, 0.55552077293396, 0.5080046057701111, 0.5409746170043945, 0.4899818003177643, 0.5351999998092651, 0.523283064365387, 0.3803611397743225, 0.46284809708595276]\n",
            "epoch_num ---->7\n",
            "[0.444100022315979, 0.41450798511505127, 0.5583475232124329]\n",
            "[0.4068310856819153, 0.4895194470882416, 0.505183756351471, 0.5645382404327393, 0.5169143676757812, 0.588818371295929, 0.5062569379806519, 0.4570028781890869, 0.5049986839294434, 0.480659157037735]\n",
            "[0.4541003108024597, 0.4555552899837494, 0.47512438893318176, 0.5023153424263, 0.3923414945602417, 0.4171154201030731, 0.4645513892173767, 0.42663541436195374, 0.4034122824668884]\n",
            "[0.44003814458847046, 0.4092966914176941]\n",
            "[0.5673137903213501, 0.43880927562713623, 0.42942333221435547, 0.5547300577163696, 0.5816456079483032, 0.4534718990325928, 0.5443101525306702, 0.4351963996887207, 0.46866098046302795, 0.4002556800842285, 0.39766985177993774, 0.5053623914718628]\n",
            "epoch_num ---->8\n",
            "[0.41062262654304504, 0.3290071189403534, 0.4604949355125427]\n",
            "[0.4693159759044647, 0.4818604290485382, 0.4317166209220886, 0.4130500853061676, 0.5070828795433044, 0.47938069701194763, 0.48240166902542114, 0.3380328118801117, 0.40170007944107056, 0.4244397282600403]\n",
            "[0.4180813431739807, 0.5183649063110352, 0.3784978985786438, 0.48859885334968567, 0.37784868478775024, 0.5741250514984131, 0.4204838275909424, 0.4583909809589386, 0.5192716121673584]\n",
            "[0.43753424286842346, 0.4863325357437134]\n",
            "[0.43966197967529297, 0.47136253118515015, 0.5082935094833374, 0.41882655024528503, 0.4650176763534546, 0.4022126793861389, 0.45744216442108154, 0.4503481984138489, 0.3914778530597687, 0.49045541882514954, 0.43868589401245117, 0.49839460849761963]\n",
            "epoch_num ---->9\n",
            "[0.3864980638027191, 0.5575072765350342, 0.46402454376220703]\n",
            "[0.44473597407341003, 0.49241432547569275, 0.46376195549964905, 0.4935169219970703, 0.48602795600891113, 0.4892842471599579, 0.5488576889038086, 0.38897705078125, 0.4344485104084015, 0.523575484752655]\n",
            "[0.44945359230041504, 0.4160880148410797, 0.44471192359924316, 0.4337017834186554, 0.44702041149139404, 0.4655916094779968, 0.5136051177978516, 0.45812180638313293, 0.5162209272384644]\n",
            "[0.4263096749782562, 0.49451836943626404]\n",
            "[0.4287818670272827, 0.46588191390037537, 0.4448583126068115, 0.5031484365463257, 0.4950563907623291, 0.4282333254814148, 0.5054839253425598, 0.3829992115497589, 0.4467838406562805, 0.458013653755188, 0.433763325214386, 0.45362550020217896]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0p4inKIO3O5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}