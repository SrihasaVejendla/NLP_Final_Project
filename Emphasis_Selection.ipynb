{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmphasisSelection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q-3MHNGoKuz",
        "outputId": "a7285496-16bd-41ca-c676-ba13d1ec6dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov  2 20:20:16 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6KNiijNYN-K"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpetzFRcWBmg"
      },
      "source": [
        "def dataset(filename):\n",
        "    with open(filename,'r') as fp:\n",
        "        lines = [line.strip() for line in fp]\n",
        "    return lines"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwPuzPYsWBp3"
      },
      "source": [
        "def wordData(data):\n",
        "    wordLines = data\n",
        "    words = []\n",
        "    probabilities = []\n",
        "    wordList = []\n",
        "    pos = []\n",
        "    empty = []\n",
        "    for line in wordLines:\n",
        "        lineSplit = line.strip().split('\\t')\n",
        "        if line:\n",
        "            word = lineSplit[1]\n",
        "            prob = lineSplit[4]\n",
        "            temp = lineSplit[5]\n",
        "            words.append(word)\n",
        "            probabilities.append(prob)\n",
        "            pos.append(temp)\n",
        "        elif not (len(empty) and []):\n",
        "            wordList.append((words, pos, probabilities))\n",
        "            words = []\n",
        "            probabilities = []\n",
        "            pos = []\n",
        "    return wordList"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ugzYtznWB1X"
      },
      "source": [
        "def wordtest(data):\n",
        "    wordLines = data\n",
        "    words = []\n",
        "    testWord = []\n",
        "    empty = []\n",
        "    for line in wordLines:\n",
        "        lineSplit = line.strip().split('\\t')\n",
        "        if line:\n",
        "            word = lineSplit[1]            \n",
        "            words.append(word)\n",
        "        elif not len(empty):\n",
        "            testWord.append(words)\n",
        "            words = []       \n",
        "    return testWord"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpkAMX2uWB3y"
      },
      "source": [
        "def preTokenizing(data):\n",
        "    t = []\n",
        "    po = []\n",
        "    prob = []\n",
        "    for i,j,k in data:\n",
        "        text = []\n",
        "        pos = []\n",
        "        probs = []\n",
        "        for l in i:\n",
        "            text.append(l)\n",
        "        for m in j:\n",
        "            pos.append(m)\n",
        "        for n in k:\n",
        "            probs.append(n)\n",
        "        t.append(text)\n",
        "        po.append(pos)\n",
        "        prob.append(probs)\n",
        "    return t,po, prob"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n7FCyJQWB7K"
      },
      "source": [
        "TRAINING_FILE = \"train.txt\"\n",
        "DEV_FILE = \"dev.txt\"\n",
        "TEST_FILE = \"test_data.txt\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBpfUuPwYHgf"
      },
      "source": [
        "trainText = wordData(dataset(TRAINING_FILE))\n",
        "testEval = wordtest(dataset(TEST_FILE))\n",
        "devText = wordData(dataset(DEV_FILE))\n",
        "\n",
        "\n",
        "trainWords,trainTags, trainLabels = preTokenizing(trainText)\n",
        "devWords, devTags, devLabels = preTokenizing(devText)\n",
        "\n",
        "tokenized_text = []\n",
        "for i in trainWords:\n",
        "    sent = \"\"\n",
        "    for j in i:\n",
        "        if sent == \"\":\n",
        "            sent += j\n",
        "        else:\n",
        "            sent = sent + \" \" + j\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    tokenized_text.append(tokens)\n",
        "    \n",
        "\n",
        "# Encoding each word of the sentence and appending to a list\n",
        "en=[]\n",
        "for i in tokenized_text:\n",
        "  en1 = []\n",
        "  for j in i:\n",
        "    token_ids = en1.append(tokenizer.encode_plus(j, add_special_tokens=False, return_attention_mask=False, return_tensors='pt'))\n",
        "  en.append(en1)\n",
        "  en1 = []\n",
        "# Getting the input id's only for the list and passing them to the model \n",
        "words = []  \n",
        "for d in en:\n",
        "    words2 = []\n",
        "    for i in d:\n",
        "        words2.append(i['input_ids'])\n",
        "    words.append(words2)\n",
        "    words2 = []     "
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa8Kf3q2rgdx"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('nghuyong/ernie-2.0-en')\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        _, pooled_output = self.bert(tokens)\n",
        "        linear_output = self.linear(pooled_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        proba = proba.data.tolist()\n",
        "        return proba"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B70NVVPoepQP",
        "outputId": "510f38c0-e376-4b28-88c3-89ade5d70232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
        "bert_clf.train()\n",
        "token_answers = []\n",
        "for epoch_num in range(1):\n",
        "    for batch_data in words:\n",
        "        answers_temp = []\n",
        "        for i in batch_data:\n",
        "            probas = bert_clf(i)\n",
        "            for item in probas:\n",
        "                for i in item:\n",
        "                    answers_temp.append(i)            \n",
        "        token_answers.append(answers_temp)\n",
        "        print(answers_temp)\n",
        "        answers_temp = []\n",
        "        bert_clf.zero_grad()\n",
        "        optimizer.step()\n",
        "        break\n",
        "      "
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.4695046544075012, 0.5315011739730835, 0.5227498412132263]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}