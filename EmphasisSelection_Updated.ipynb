{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSLKZ2lcJx06",
    "outputId": "d3e68ff4-111a-45c6-dafc-e88079b4093c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mithi\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: filelock in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.19.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.5->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the transformers for using pre-trained Models\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "id": "s6KNiijNYN-K"
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the tokenizer and pre_trained model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "pre_trained_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "id": "rpetzFRcWBmg"
   },
   "outputs": [],
   "source": [
    "# Load the train, test and dev dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename,'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "id": "qwPuzPYsWBp3"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Train and Dev dataset\n",
    "def word_traindev_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    probabilities = []\n",
    "    wordList = []\n",
    "    pos = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]\n",
    "            prob = lineSplit[4]\n",
    "            temp = lineSplit[5]\n",
    "            words.append(word)\n",
    "            probabilities.append(float(prob))\n",
    "            pos.append(temp)\n",
    "        elif not (len(empty) and []):\n",
    "            wordList.append((words, pos, probabilities))\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            pos = []\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "id": "1ugzYtznWB1X"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Test dataset\n",
    "def word_test_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    testWord = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]            \n",
    "            words.append(word)\n",
    "        elif not len(empty):\n",
    "            testWord.append(words)\n",
    "            words = []       \n",
    "    return testWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "id": "hpkAMX2uWB3y"
   },
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Train and Dev data\n",
    "def data_preprocess_train_dev(data):\n",
    "    text = []\n",
    "    pos = []\n",
    "    probs = []\n",
    "    for i,j,k in data:\n",
    "            text.append(i)\n",
    "            pos.append(j)\n",
    "            probs.append(k)\n",
    "    return text,pos, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Test data\n",
    "def data_preprocess_test(data):\n",
    "    text = []\n",
    "    for i in data:\n",
    "            text.append(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicating probablities for matching length incase of sub tokenized words\n",
    "def prob_list(batch_data,batch_probs):\n",
    "    pb = []\n",
    "    for i,j in zip(batch_data,batch_probs):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(float(l))\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(float(l))\n",
    "        pb.append(tp)\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence from words in dataset\n",
    "def get_sentence(words):    \n",
    "    tokenized_text = []\n",
    "    for i in words:\n",
    "        sent = ''\n",
    "        for h in i:\n",
    "            if sent == '':\n",
    "                sent = sent + h\n",
    "            else:\n",
    "                sent = sent+ \" \" +h\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tid = tokenizer.encode(tokens, add_special_tokens=False)\n",
    "        tokenized_text.append(tid)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_func(data):\n",
    "    max_len = 0\n",
    "    for i in data:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = [i + [0]*(max_len-len(i)) for i in data]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "id": "6n7FCyJQWB7K"
   },
   "outputs": [],
   "source": [
    "# Specifying file names\n",
    "TRAINING_FILE = \"train.txt\"\n",
    "DEV_FILE = \"dev.txt\"\n",
    "TEST_FILE = \"test_data.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "id": "uBpfUuPwYHgf"
   },
   "outputs": [],
   "source": [
    "# Preprocessing work on the dataset \n",
    "trainText = word_traindev_Data(load_dataset(TRAINING_FILE))\n",
    "testEval = word_test_Data(load_dataset(TEST_FILE))\n",
    "devText = word_traindev_Data(load_dataset(DEV_FILE))\n",
    "\n",
    "trainWords,trainTags, trainLabels = data_preprocess_train_dev(trainText)\n",
    "devWords, devTags, devLabels = data_preprocess_train_dev(devText)\n",
    "testWords = data_preprocess_test(testEval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "train_tokens = get_sentence(trainWords)\n",
    "train_probablities = prob_list(trainWords,trainLabels)\n",
    "train_tokens_pad = pad_func(train_tokens)\n",
    "train_probablities_pad = pad_func(train_probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dev data\n",
    "dev_tokens = get_sentence(devWords)\n",
    "dev_probablities = prob_list(devWords,devLabels)\n",
    "dev_tokens_pad = pad_func(dev_tokens)\n",
    "dev_probablities_pad = pad_func(dev_probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "id": "xa8Kf3q2rgdx"
   },
   "outputs": [],
   "source": [
    "#defining the model class\n",
    "class ErnieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErnieModel, self).__init__()\n",
    "        self.ernie = pre_trained_model\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pooled_output,_ = self.ernie(tokens)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70NVVPoepQP",
    "outputId": "739a312f-19c5-4a3d-ed60-4219eb639586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch number ---->0\n",
      "Training loss ---->0.21222754567861557\n",
      "Total runtime ----> 1746.43834066391 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 68.5400459766388 seconds\n",
      "\n",
      "Running epoch number ---->1\n",
      "Training loss ---->0.21182132991296904\n",
      "Total runtime ----> 1700.1604647636414 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 68.23428678512573 seconds\n",
      "\n",
      "Running epoch number ---->2\n",
      "Training loss ---->0.21185098003063882\n",
      "Total runtime ----> 1696.9388134479523 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 72.30727744102478 seconds\n",
      "\n",
      "Running epoch number ---->3\n",
      "Training loss ---->0.21213645062276296\n",
      "Total runtime ----> 1652.2579152584076 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 64.92111802101135 seconds\n",
      "\n",
      "Running epoch number ---->4\n",
      "Training loss ---->0.21288843133619853\n",
      "Total runtime ----> 1724.8364732265472 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 61.482586145401 seconds\n",
      "\n",
      "Running epoch number ---->5\n",
      "Training loss ---->0.21183172879474504\n",
      "Total runtime ----> 1774.2070286273956 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 61.13309383392334 seconds\n",
      "\n",
      "Running epoch number ---->6\n",
      "Training loss ---->0.21129911605800902\n",
      "Total runtime ----> 1791.594302892685 seconds\n",
      "\n",
      "Validation loss ---->0.19990291446447372\n",
      "Total runtime ----> 64.42729258537292 seconds\n",
      "\n",
      "Running epoch number ---->7\n",
      "Training loss ---->0.2116740643978119\n",
      "Total runtime ----> 1805.3177587985992 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model = ErnieModel()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.1)\n",
    "loss_func = nn.MSELoss(reduction = 'mean')\n",
    "batch = 100\n",
    "for epoch_num in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    print(\"Running epoch number ---->{}\".format(epoch_num))\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(0, len(train_tokens_pad), batch):\n",
    "        model.zero_grad()\n",
    "        train_probas = model(torch.tensor(train_tokens_pad[i:i+batch]))\n",
    "        train_grd_truth = []\n",
    "        for i in train_probablities_pad[i:i+batch]:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            train_grd_truth.append(p)\n",
    "        train_batch_loss = loss_func(train_probas, torch.tensor(train_grd_truth))\n",
    "        training_loss.append(train_batch_loss.item())\n",
    "        train_batch_loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "    print(\"Training loss ---->{}\".format((np.average(training_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    \n",
    "    #Validation Run\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(dev_tokens_pad), batch):\n",
    "        dev_probas = model(torch.tensor(dev_tokens_pad[i:i+batch]))\n",
    "        dev_grd_truth = []\n",
    "        for i in dev_probablities_pad[i:i+batch]:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            dev_grd_truth.append(p)\n",
    "        dev_batch_loss = loss_func(dev_probas, torch.tensor(dev_grd_truth))\n",
    "        validation_loss.append(dev_batch_loss.item())\n",
    "    print(\"Validation loss ---->{}\".format((np.average(validation_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "filename = 'emphasis_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmphasisSelection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
