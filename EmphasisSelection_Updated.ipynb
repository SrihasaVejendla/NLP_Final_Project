{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSLKZ2lcJx06",
    "outputId": "d3e68ff4-111a-45c6-dafc-e88079b4093c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mithi\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.19.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.5->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the transformers for using pre-trained Models\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s6KNiijNYN-K"
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "from earlystopping import EarlyStopping\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the tokenizer and pre_trained model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "pre_trained_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rpetzFRcWBmg"
   },
   "outputs": [],
   "source": [
    "# Load the train, test and dev dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename,'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qwPuzPYsWBp3"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Train and Dev dataset\n",
    "def word_traindev_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    probabilities = []\n",
    "    wordList = []\n",
    "    pos = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]\n",
    "            prob = lineSplit[4]\n",
    "            temp = lineSplit[5]\n",
    "            words.append(word)\n",
    "            probabilities.append(float(prob))\n",
    "            pos.append(temp)\n",
    "        elif not (len(empty) and []):\n",
    "            wordList.append((words, pos, probabilities))\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            pos = []\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1ugzYtznWB1X"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Test dataset\n",
    "def word_test_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    testWord = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]            \n",
    "            words.append(word)\n",
    "        elif not len(empty):\n",
    "            testWord.append(words)\n",
    "            words = []       \n",
    "    return testWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hpkAMX2uWB3y"
   },
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Train and Dev data\n",
    "def data_preprocess_train_dev(data):\n",
    "    text = []\n",
    "    pos = []\n",
    "    probs = []\n",
    "    for i,j,k in data:\n",
    "            text.append(i)\n",
    "            pos.append(j)\n",
    "            probs.append(k)\n",
    "    return text,pos, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Test data\n",
    "def data_preprocess_test(data):\n",
    "    text = []\n",
    "    for i in data:\n",
    "            text.append(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicating probablities for matching length incase of sub tokenized words\n",
    "def prob_list(batch_data,batch_probs):\n",
    "    pb = []\n",
    "    for i,j in zip(batch_data,batch_probs):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(float(l))\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(float(l))\n",
    "        pb.append(tp)\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence from words in dataset\n",
    "def get_sentence(words):    \n",
    "    tokenized_text = []\n",
    "    for i in words:\n",
    "        sent = ''\n",
    "        for h in i:\n",
    "            if sent == '':\n",
    "                sent = sent + h\n",
    "            else:\n",
    "                sent = sent+ \" \" +h\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tid = tokenizer.encode(tokens, add_special_tokens=False)\n",
    "        tokenized_text.append(tid)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_func(data):\n",
    "    max_len = 0\n",
    "    for i in data:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = [i + [0]*(max_len-len(i)) for i in data]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6n7FCyJQWB7K"
   },
   "outputs": [],
   "source": [
    "# Specifying file names\n",
    "TRAINING_FILE = \"train.txt\"\n",
    "DEV_FILE = \"dev.txt\"\n",
    "TEST_FILE = \"test_data.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uBpfUuPwYHgf"
   },
   "outputs": [],
   "source": [
    "# Preprocessing work on the dataset \n",
    "trainText = word_traindev_Data(load_dataset(TRAINING_FILE))\n",
    "testEval = word_test_Data(load_dataset(TEST_FILE))\n",
    "devText = word_traindev_Data(load_dataset(DEV_FILE))\n",
    "\n",
    "trainWords,trainTags, trainLabels = data_preprocess_train_dev(trainText)\n",
    "devWords, devTags, devLabels = data_preprocess_train_dev(devText)\n",
    "testWords = data_preprocess_test(testEval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "train_tokens = get_sentence(trainWords)\n",
    "train_probablities = prob_list(trainWords,trainLabels)\n",
    "train_tokens_pad = pad_func(train_tokens)\n",
    "train_probablities_pad = pad_func(train_probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dev data\n",
    "dev_tokens = get_sentence(devWords)\n",
    "dev_probablities = prob_list(devWords,devLabels)\n",
    "dev_tokens_pad = pad_func(dev_tokens)\n",
    "dev_probablities_pad = pad_func(dev_probablities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xa8Kf3q2rgdx"
   },
   "outputs": [],
   "source": [
    "#defining the model class\n",
    "class ErnieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErnieModel, self).__init__()\n",
    "        self.ernie = pre_trained_model\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pooled_output,_ = self.ernie(tokens)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70NVVPoepQP",
    "outputId": "739a312f-19c5-4a3d-ed60-4219eb639586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch number ---->0\n",
      "Training loss ---->0.22760500412347706\n",
      "Total runtime ----> 1451.2978570461273 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 41.52866888046265 seconds\n",
      "\n",
      "Validation loss is (inf --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->1\n",
      "Training loss ---->0.2268183042490205\n",
      "Total runtime ----> 1444.3351719379425 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 39.96937441825867 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->2\n",
      "Training loss ---->0.22727009550083516\n",
      "Total runtime ----> 1411.040531873703 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 39.42757558822632 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->3\n",
      "Training loss ---->0.22665972921044328\n",
      "Total runtime ----> 1407.9004814624786 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 38.54274368286133 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->4\n",
      "Training loss ---->0.2275205162722011\n",
      "Total runtime ----> 1376.9920139312744 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 37.925410747528076 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->5\n",
      "Training loss ---->0.22630752900312112\n",
      "Total runtime ----> 1370.6015536785126 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 38.25303936004639 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->6\n",
      "Training loss ---->0.22687748369089392\n",
      "Total runtime ----> 1381.6672868728638 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 40.27478528022766 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->7\n",
      "Training loss ---->0.22674895008636076\n",
      "Total runtime ----> 1375.3411793708801 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 37.43525171279907 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->8\n",
      "Training loss ---->0.2266766217558883\n",
      "Total runtime ----> 1376.1826360225677 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 39.40752720832825 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n",
      "Running epoch number ---->9\n",
      "Training loss ---->0.22686237997786943\n",
      "Total runtime ----> 1372.6901786327362 seconds\n",
      "\n",
      "Validation loss ---->0.21707046031951904\n",
      "Total runtime ----> 37.648115396499634 seconds\n",
      "\n",
      "Validation loss is (0.21707 --> 0.21707).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model = ErnieModel()\n",
    "model_path = 'emphasismodel.pth'\n",
    "early_stopping = EarlyStopping(model_path,4,True)\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.1)\n",
    "loss_func = nn.MSELoss(reduction = 'mean')\n",
    "batch = 32\n",
    "for epoch_num in range(10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    print(\"Running epoch number ---->{}\".format(epoch_num))\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(0, len(train_tokens_pad), batch):\n",
    "        model.zero_grad()\n",
    "        train_probas = model(torch.tensor(train_tokens_pad[i:i+batch]))\n",
    "        train_grd_truth = []\n",
    "        for i in train_probablities_pad[i:i+batch]:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            train_grd_truth.append(p)\n",
    "        train_batch_loss = loss_func(train_probas, torch.tensor(train_grd_truth))\n",
    "        training_loss.append(train_batch_loss.item())\n",
    "        train_batch_loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Training loss ---->{}\".format((np.average(training_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    \n",
    "    #Validation Run\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(dev_tokens_pad), batch):\n",
    "        dev_probas = model(torch.tensor(dev_tokens_pad[i:i+batch]))\n",
    "        dev_grd_truth = []\n",
    "        for i in dev_probablities_pad[i:i+batch]:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            dev_grd_truth.append(p)\n",
    "        dev_batch_loss = loss_func(dev_probas, torch.tensor(dev_grd_truth))\n",
    "        validation_loss.append(dev_batch_loss.item())\n",
    "        \n",
    "    print(\"Validation loss ---->{}\".format((np.average(validation_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    early_stopping(np.average(validation_loss), model)\n",
    "\n",
    "    if early_stopping.early_stop is True:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "torch.save(model,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the trained model\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "tokenized_test_text = []\n",
    "for i in testWords:\n",
    "    sent = \"\"\n",
    "    for j in i:\n",
    "        if sent == \"\":\n",
    "            sent += j\n",
    "        else:\n",
    "            sent = sent + \" \" + j\n",
    "    tokenized_test_text.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on Test Dataset\n",
    "test_prob=[]\n",
    "\n",
    "for batch_data in tokenized_test_text:\n",
    "    tokens = tokenizer.tokenize(batch_data)\n",
    "    tid = tokenizer.encode_plus(tokens, add_special_tokens=False, return_attention_mask=False, return_tensors='pt')\n",
    "    test_probas = model(tid['input_ids'])\n",
    "    test_probas=test_probas.data\n",
    "    out = batch_data.split(\" \")\n",
    "    temp_ans = []\n",
    "    index = 0\n",
    "    for i in out:\n",
    "        if (len(tokenizer.tokenize(i))) == 1:\n",
    "            temp_ans.append(test_probas[0][index].item())\n",
    "            index = index + 1\n",
    "        else:\n",
    "            holder = []\n",
    "            for j in range(len(tokenizer.tokenize(i))):\n",
    "                holder.append(test_probas[0][index].item())\n",
    "                index = index + 1\n",
    "            prb = np.average(holder)\n",
    "            temp_ans.append(prb) \n",
    "    test_prob.append(temp_ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = We 'll be closed from 12/24 to 1/1 . See you in the New Year !\n",
      "Emphasis Values = [0.44879353046417236, 0.466549813747406, 0.5147578120231628, 0.44036665558815, 0.489033579826355, 0.4737546344598134, 0.48464372754096985, 0.4211202661196391, 0.4757114350795746, 0.4624374508857727, 0.43629321455955505, 0.523070216178894, 0.5038394331932068, 0.470909059047699, 0.5153239965438843, 0.45707380771636963]\n",
      "\n",
      "Sentence = No matter how hard you work , someone else is working harder .\n",
      "Emphasis Values = [0.38491129875183105, 0.3850019574165344, 0.5098055005073547, 0.4513833224773407, 0.40719500184059143, 0.48779892921447754, 0.45820364356040955, 0.39396074414253235, 0.47681671380996704, 0.3848966956138611, 0.4817645251750946, 0.5142143964767456, 0.38491830229759216]\n",
      "\n",
      "Sentence = The less I needed , the better I felt .\n",
      "Emphasis Values = [0.46332982182502747, 0.4040818512439728, 0.44980910420417786, 0.5157061219215393, 0.49623650312423706, 0.4885362386703491, 0.45792195200920105, 0.45895540714263916, 0.4908824563026428, 0.3445151746273041]\n",
      "\n",
      "Sentence = When someone shows you who they are , believe them the first time .\n",
      "Emphasis Values = [0.47215065360069275, 0.4739573001861572, 0.47419601678848267, 0.4481554627418518, 0.4793816804885864, 0.46626514196395874, 0.48070454597473145, 0.49034634232521057, 0.47389957308769226, 0.48066478967666626, 0.49137601256370544, 0.44101884961128235, 0.4570077657699585, 0.4739099442958832]\n",
      "\n",
      "Sentence = You are not a drop in the ocean . You are the entire ocean in a drop .\n",
      "Emphasis Values = [0.46292978525161743, 0.35762983560562134, 0.456794798374176, 0.4555600583553314, 0.356058806180954, 0.4525981545448303, 0.45641812682151794, 0.4894169867038727, 0.42811694741249084, 0.3556978702545166, 0.4766978323459625, 0.4910675287246704, 0.532541036605835, 0.4920176565647125, 0.46979811787605286, 0.4712674617767334, 0.4513269364833832, 0.44210582971572876]\n",
      "\n",
      "Sentence = All successes begin with self-discipline . It starts with you .\n",
      "Emphasis Values = [0.46067237854003906, 0.46133604645729065, 0.46118924021720886, 0.41859397292137146, 0.4727667570114136, 0.4602195620536804, 0.4608360528945923, 0.4610828757286072, 0.4017989933490753, 0.460857629776001, 0.4611198902130127]\n",
      "\n",
      "Sentence = It 's not a question of learning much . On the contrary . It 's a question of unlearning much .\n",
      "Emphasis Values = [0.45685118436813354, 0.3917142301797867, 0.45592376589775085, 0.40579137206077576, 0.3715882897377014, 0.41583240032196045, 0.5330738425254822, 0.4078839421272278, 0.4193369746208191, 0.4442017078399658, 0.45613202452659607, 0.5300062894821167, 0.4198096692562103, 0.4371168911457062, 0.4138134717941284, 0.4539485573768616, 0.3929769992828369, 0.4189320504665375, 0.4926458150148392, 0.4409903287887573, 0.44906309247016907]\n",
      "\n",
      "Sentence = When I count my blessings , I count you twice .\n",
      "Emphasis Values = [0.4545161724090576, 0.3692556619644165, 0.36320480704307556, 0.46227380633354187, 0.4854559004306793, 0.44907572865486145, 0.41114187240600586, 0.3614553213119507, 0.37061381340026855, 0.3767818808555603, 0.3750966787338257]\n",
      "\n",
      "Sentence = Feel the fear and do it anyway .\n",
      "Emphasis Values = [0.4355027675628662, 0.435762882232666, 0.4357627332210541, 0.4366839528083801, 0.43558603525161743, 0.4599856734275818, 0.434684693813324, 0.43572622537612915]\n",
      "\n",
      "Sentence = A warm smile is the universal language of kindness .\n",
      "Emphasis Values = [0.4716328978538513, 0.4603457450866699, 0.4766574203968048, 0.47401198744773865, 0.5024442076683044, 0.48568761348724365, 0.44541531801223755, 0.49911755323410034, 0.4817091226577759, 0.4741515815258026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicted Probablities\n",
    "for i in range(10):\n",
    "    print(\"Sentence = {}\".format(tokenized_test_text[i]))\n",
    "    print(\"Emphasis Values = {}\\n\".format(test_prob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmphasisSelection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
