{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSLKZ2lcJx06",
    "outputId": "d3e68ff4-111a-45c6-dafc-e88079b4093c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mithi\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: numpy in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.16.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.5->boto3->transformers) (2.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "s6KNiijNYN-K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rpetzFRcWBmg"
   },
   "outputs": [],
   "source": [
    "def dataset(filename):\n",
    "    with open(filename,'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qwPuzPYsWBp3"
   },
   "outputs": [],
   "source": [
    "def wordData(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    probabilities = []\n",
    "    wordList = []\n",
    "    pos = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]\n",
    "            prob = lineSplit[4]\n",
    "            temp = lineSplit[5]\n",
    "            words.append(word)\n",
    "            probabilities.append(float(prob))\n",
    "            pos.append(temp)\n",
    "        elif not (len(empty) and []):\n",
    "            wordList.append((words, pos, probabilities))\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            pos = []\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1ugzYtznWB1X"
   },
   "outputs": [],
   "source": [
    "def wordtest(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    testWord = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]            \n",
    "            words.append(word)\n",
    "        elif not len(empty):\n",
    "            testWord.append(words)\n",
    "            words = []       \n",
    "    return testWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hpkAMX2uWB3y"
   },
   "outputs": [],
   "source": [
    "def preTokenizing(data):\n",
    "    text = []\n",
    "    pos = []\n",
    "    probs = []\n",
    "    for i,j,k in data:\n",
    "            text.append(i)\n",
    "            pos.append(j)\n",
    "            probs.append(k)\n",
    "    return text,pos, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6n7FCyJQWB7K"
   },
   "outputs": [],
   "source": [
    "TRAINING_FILE = \"train.txt\"\n",
    "DEV_FILE = \"dev.txt\"\n",
    "TEST_FILE = \"test_data.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uBpfUuPwYHgf"
   },
   "outputs": [],
   "source": [
    "trainText = wordData(dataset(TRAINING_FILE))\n",
    "testEval = wordtest(dataset(TEST_FILE))\n",
    "devText = wordData(dataset(DEV_FILE))\n",
    "\n",
    "trainWords,trainTags, trainLabels = preTokenizing(trainText)\n",
    "devWords, devTags, devLabels = preTokenizing(devText)\n",
    "\n",
    "tokenized_text = []\n",
    "for i in trainWords:\n",
    "  sent = ''\n",
    "  for h in i:\n",
    "    if sent == '':\n",
    "      sent = sent + h\n",
    "    else:\n",
    "      sent = sent+ \" \" +h\n",
    "  tokenized_text.append(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ivcvE9YYQKYN"
   },
   "outputs": [],
   "source": [
    "def prob_list(batch_data,batch_probs):\n",
    "  pb = []\n",
    "  for i,j in zip(batch_data,batch_probs):\n",
    "    tp = []\n",
    "    for k,l in zip(i,j):\n",
    "      temp = tokenizer.tokenize(k)\n",
    "      if len(temp) == 1:\n",
    "        tp.append(float(l))\n",
    "      if len(temp) > 1:\n",
    "        for i in range(len(temp)):\n",
    "          tp.append(float(l))\n",
    "    pb.append(tp)\n",
    "  return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xa8Kf3q2rgdx"
   },
   "outputs": [],
   "source": [
    "class ErnieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErnieModel, self).__init__()\n",
    "        self.ernie = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        pooled_output,_ = self.ernie(tokens)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70NVVPoepQP",
    "outputId": "739a312f-19c5-4a3d-ed60-4219eb639586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch_num ---->0.....\n",
      "loss ---->0.1414060562849045\n",
      "Total runtime ----> 37.41793632507324 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_clf = ErnieModel()\n",
    "optimizer = optim.Adamax(bert_clf.parameters(), lr=0.1)\n",
    "bert_clf.train()\n",
    "probablities = prob_list(trainWords,trainLabels)\n",
    "pred_probs = []\n",
    "for epoch_num in range(1):\n",
    "    start_time = time.time()\n",
    "    print(\"Running epoch_num ---->{}.....\".format(epoch_num))\n",
    "    count = 0\n",
    "    ep_ls = 0.0\n",
    "    for batch_data, batch_probs in zip(tokenized_text, probablities):\n",
    "      bert_clf.zero_grad()\n",
    "      answers_temp = []\n",
    "      tokens = tokenizer.tokenize(batch_data)\n",
    "      tid = tokenizer.encode_plus(tokens, add_special_tokens=False, return_attention_mask=False, return_tensors='pt')\n",
    "      probas = bert_clf(tid['input_ids'])\n",
    "      loss_func = nn.MSELoss(reduction = 'mean')\n",
    "      p = []\n",
    "      for i in batch_probs:\n",
    "        q=[]\n",
    "        q.append(i)\n",
    "        p.append(q)\n",
    "      batch_loss = loss_func(probas, torch.tensor(p))\n",
    "      ep_ls = ep_ls + batch_loss.item()\n",
    "      batch_loss.backward()\n",
    "      optimizer.zero_grad()\n",
    "      optimizer.step()\n",
    "      count = count + 1\n",
    "      o = batch_data.split(\" \")\n",
    "      temp_ans = []\n",
    "      k = 0\n",
    "      for i in o:\n",
    "        if (len(tokenizer.tokenize(i))) == 1:\n",
    "            temp_ans.append(probas[0][k].item())\n",
    "            k= k + 1\n",
    "        else:\n",
    "            dum = []\n",
    "            for g in range(len(tokenizer.tokenize(i))):\n",
    "               dum.append(probas[0][k].item())\n",
    "               k = k + 1\n",
    "            val = np.average(dum)\n",
    "            temp_ans.append(val)   \n",
    "      pred_probs.append(temp_ans)\n",
    "      if count == 10:\n",
    "        break\n",
    "    print(\"loss ---->{}\".format((ep_ls/float(count))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'modelep_5.sav'\n",
    "pickle.dump(bert_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5728209614753723, 0.5750526189804077, 0.5087984204292297],\n",
       " [0.48095619678497314,\n",
       "  0.4743928015232086,\n",
       "  0.5327974557876587,\n",
       "  0.5838488340377808,\n",
       "  0.5195691585540771,\n",
       "  0.4967581629753113,\n",
       "  0.47870853543281555,\n",
       "  0.5170894861221313,\n",
       "  0.5010387897491455,\n",
       "  0.4500127136707306],\n",
       " [0.6143060922622681,\n",
       "  0.46975140273571014,\n",
       "  0.5745329856872559,\n",
       "  0.608700156211853,\n",
       "  0.5496838092803955,\n",
       "  0.5981898903846741,\n",
       "  0.5987788438796997,\n",
       "  0.5419473648071289],\n",
       " [0.4919780194759369, 0.4203195571899414],\n",
       " [0.4081747233867645,\n",
       "  0.4090423583984375,\n",
       "  0.4211327135562897,\n",
       "  0.43705207109451294,\n",
       "  0.36064231395721436,\n",
       "  0.5978460907936096,\n",
       "  0.43168124556541443,\n",
       "  0.4607680141925812,\n",
       "  0.4083693027496338,\n",
       "  0.45330724120140076,\n",
       "  0.4535541832447052,\n",
       "  0.5563782453536987],\n",
       " [0.6730214357376099,\n",
       "  0.6171466708183289,\n",
       "  0.5863562226295471,\n",
       "  0.6619576215744019,\n",
       "  0.5785463452339172,\n",
       "  0.5180293321609497,\n",
       "  0.5949971079826355,\n",
       "  0.5721586346626282,\n",
       "  0.5892332792282104,\n",
       "  0.5543256402015686,\n",
       "  0.5643171668052673,\n",
       "  0.6531903743743896,\n",
       "  0.552983283996582,\n",
       "  0.529422402381897,\n",
       "  0.5999807715415955,\n",
       "  0.5444698929786682,\n",
       "  0.578845739364624,\n",
       "  0.6248984932899475,\n",
       "  0.4736344516277313],\n",
       " [0.6047669649124146,\n",
       "  0.5502322316169739,\n",
       "  0.5799843668937683,\n",
       "  0.5901085138320923],\n",
       " [0.5975145697593689,\n",
       "  0.5139859914779663,\n",
       "  0.5659923553466797,\n",
       "  0.6226819157600403,\n",
       "  0.6198990345001221,\n",
       "  0.621103823184967,\n",
       "  0.5797211527824402,\n",
       "  0.6294687390327454,\n",
       "  0.6093364953994751,\n",
       "  0.5673118829727173],\n",
       " [0.45777422189712524,\n",
       "  0.4567735493183136,\n",
       "  0.5673759778340658,\n",
       "  0.5186148881912231,\n",
       "  0.4932697117328644,\n",
       "  0.49660277366638184,\n",
       "  0.5652942657470703,\n",
       "  0.5516973932584127,\n",
       "  0.5182690024375916],\n",
       " [0.4371998608112335,\n",
       "  0.5974387526512146,\n",
       "  0.5518196225166321,\n",
       "  0.5960565805435181,\n",
       "  0.5954410433769226,\n",
       "  0.5875838398933411,\n",
       "  0.5713174343109131,\n",
       "  0.6166419982910156,\n",
       "  0.59345543384552,\n",
       "  0.5594140887260437,\n",
       "  0.5778266787528992,\n",
       "  0.5692886114120483,\n",
       "  0.6229850053787231,\n",
       "  0.5729642510414124,\n",
       "  0.5103780627250671]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmphasisSelection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
