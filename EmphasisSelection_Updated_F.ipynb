{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSLKZ2lcJx06",
    "outputId": "d3e68ff4-111a-45c6-dafc-e88079b4093c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mithi\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.16.5)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: requests in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.19.5)\n",
      "Requirement already satisfied: click in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mithi\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.5->boto3->transformers) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "#Install the transformers for using pre-trained Models\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s6KNiijNYN-K"
   },
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import warnings\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from earlystopping import EarlyStopping\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the tokenizer and pre_trained model \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "pre_trained_model = AutoModel.from_pretrained('nghuyong/ernie-2.0-large-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rpetzFRcWBmg"
   },
   "outputs": [],
   "source": [
    "# Load the train, test and dev dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename,'r') as fp:\n",
    "        lines = [line.strip() for line in fp]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qwPuzPYsWBp3"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Train and Dev dataset\n",
    "def word_traindev_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    probabilities = []\n",
    "    wordList = []\n",
    "    pos = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]\n",
    "            prob = lineSplit[4]\n",
    "            temp = lineSplit[5]\n",
    "            words.append(word)\n",
    "            probabilities.append(float(prob))\n",
    "            pos.append(temp)\n",
    "        elif not (len(empty) and []):\n",
    "            wordList.append((words, pos, probabilities))\n",
    "            words = []\n",
    "            probabilities = []\n",
    "            pos = []\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1ugzYtznWB1X"
   },
   "outputs": [],
   "source": [
    "# Getting the words, pos, probablities from both the Test dataset\n",
    "def word_test_Data(data):\n",
    "    wordLines = data\n",
    "    words = []\n",
    "    testWord = []\n",
    "    empty = []\n",
    "    for line in wordLines:\n",
    "        lineSplit = line.strip().split('\\t')\n",
    "        if line:\n",
    "            word = lineSplit[1]            \n",
    "            words.append(word)\n",
    "        elif not len(empty):\n",
    "            testWord.append(words)\n",
    "            words = []       \n",
    "    return testWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hpkAMX2uWB3y"
   },
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Train and Dev data\n",
    "def data_preprocess_train_dev(data):\n",
    "    text = []\n",
    "    pos = []\n",
    "    probs = []\n",
    "    for i,j,k in data:\n",
    "            text.append(i)\n",
    "            pos.append(j)\n",
    "            probs.append(k)\n",
    "    return text,pos, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate separate list of words, pos and probablities for Test data\n",
    "def data_preprocess_test(data):\n",
    "    text = []\n",
    "    for i in data:\n",
    "            text.append(i)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicating probablities for matching length incase of sub tokenized words\n",
    "def prob_list(batch_data,batch_probs):\n",
    "    pb = []\n",
    "    for i,j in zip(batch_data,batch_probs):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(float(l))\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(float(l))\n",
    "        pb.append(tp)\n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicating feature vectors for matching length incase of sub tokenized words\n",
    "def feature_list(batch_data,feature):\n",
    "    fv = []\n",
    "    for i,j in zip(batch_data,feature):\n",
    "        tp = []\n",
    "        for k,l in zip(i,j):\n",
    "            temp = tokenizer.tokenize(k)\n",
    "            if len(temp) == 1:\n",
    "                tp.append(l)\n",
    "            if len(temp) > 1:\n",
    "                for i in range(len(temp)):\n",
    "                    tp.append(l)\n",
    "        fv.append(tp)\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence from words in dataset\n",
    "def get_sentence(words):    \n",
    "    tokenized_text = []\n",
    "    for i in words:\n",
    "        sent = ''\n",
    "        for h in i:\n",
    "            if sent == '':\n",
    "                sent = sent + h\n",
    "            else:\n",
    "                sent = sent+ \" \" +h\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tid = tokenizer.encode(tokens, add_special_tokens=False)\n",
    "        tokenized_text.append(tid)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pad data for equal length\n",
    "def pad_func(data):\n",
    "    max_len = 0\n",
    "    for i in data:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    if type(i[0]) is list:\n",
    "        padded = [i + [[0, 0, 0, 0]]*(max_len-len(i)) for i in data]\n",
    "    else:\n",
    "        padded = [i + [0]*(max_len-len(i)) for i in data]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation function\n",
    "def data_augment(words, probs):\n",
    "    aug_word_list = []\n",
    "    aug_prob_list = []\n",
    "    for i in range(len(words)):\n",
    "        aug_word_list.append(words[i])\n",
    "        aug_prob_list.append(probs[i])\n",
    "        \n",
    "        if (i%2) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word.reverse()\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            temp_prb.reverse()\n",
    "            aug_prob_list.append(temp_prb)\n",
    "            \n",
    "        if (i%3) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word[0] = temp_word[0].upper()\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            aug_prob_list.append(temp_prb)\n",
    "            \n",
    "        if (i%5) == 0:\n",
    "            temp_word = copy.copy(words[i])\n",
    "            temp_word.remove(temp_word[0])\n",
    "            aug_word_list.append(temp_word)\n",
    "            \n",
    "            \n",
    "            temp_prb = copy.copy(probs[i])\n",
    "            temp_prb.remove(temp_prb[0])\n",
    "            aug_prob_list.append(temp_prb)\n",
    "                \n",
    "    return aug_word_list, aug_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature vector for the words\n",
    "def feature_add(trainWords):\n",
    "    feature = []\n",
    "    for i in trainWords:\n",
    "        temp1 = []\n",
    "        for j in i:\n",
    "            temp2 =[0] * 4\n",
    "            if j[0].isupper():\n",
    "                temp2[0] = 1\n",
    "            else:\n",
    "                temp2[0] = 0\n",
    "            if '#' in j:\n",
    "                temp2[1] = 1\n",
    "            else:\n",
    "                temp2[1] = 0\n",
    "            if j.isupper():\n",
    "                temp2[2] = 1\n",
    "            else:\n",
    "                temp2[2] = 0\n",
    "            if len(tokenizer.tokenize(j))>1:\n",
    "                temp2[3] = 1\n",
    "            else:\n",
    "                temp2[3] = 0\n",
    "            temp1.append(temp2)\n",
    "        feature.append(temp1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to shuffle the dataset \n",
    "def func_shuffle(tokens, probablities, feature, attention):\n",
    "    mapIndexPosition = list(zip(tokens, probablities, feature, attention))\n",
    "    np.random.shuffle(mapIndexPosition)\n",
    "    tokens, probablities, feature, attention = zip(*mapIndexPosition)\n",
    "    return tokens, probablities, feature, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get attention mask\n",
    "def gen_attention(data):\n",
    "    attention_mask = []\n",
    "    for i in data:\n",
    "        tmp = list([1] * (np.count_nonzero(i))) + list([0] * (len(i) - (np.count_nonzero(i))))\n",
    "        attention_mask.append(tmp)\n",
    "    return attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6n7FCyJQWB7K"
   },
   "outputs": [],
   "source": [
    "# Specifying file names\n",
    "TRAINING_FILE = \"train.txt\"\n",
    "DEV_FILE = \"dev.txt\"\n",
    "TEST_FILE = \"test_data.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uBpfUuPwYHgf"
   },
   "outputs": [],
   "source": [
    "# Preprocessing work on the dataset \n",
    "trainText = word_traindev_Data(load_dataset(TRAINING_FILE))\n",
    "testEval = word_test_Data(load_dataset(TEST_FILE))\n",
    "devText = word_traindev_Data(load_dataset(DEV_FILE))\n",
    "\n",
    "trainWords,trainTags, trainLabels = data_preprocess_train_dev(trainText)\n",
    "devWords, devTags, devLabels = data_preprocess_train_dev(devText)\n",
    "testWords = data_preprocess_test(testEval)\n",
    "\n",
    "trainWords, trainLabels = data_augment(trainWords, trainLabels)\n",
    "devWords, devLabels = data_augment(devWords, devLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "train_tokens = get_sentence(trainWords)\n",
    "train_probablities = prob_list(trainWords,trainLabels)\n",
    "train_features = feature_add(trainWords)\n",
    "train_feature = feature_list(trainWords,train_features)\n",
    "\n",
    "train_tokens_pad = pad_func(train_tokens)\n",
    "train_probablities_pad = pad_func(train_probablities)\n",
    "train_feature_pad = pad_func(train_feature)\n",
    "train_attention = gen_attention(train_tokens_pad)\n",
    "\n",
    "train_tokens_pad, train_probablities_pad, train_feature_pad, train_attention = func_shuffle(train_tokens_pad, train_probablities_pad, train_feature_pad, train_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dev data\n",
    "dev_tokens = get_sentence(devWords)\n",
    "dev_probablities = prob_list(devWords,devLabels)\n",
    "dev_features = feature_add(devWords)\n",
    "dev_feature = feature_list(devWords,dev_features)\n",
    "\n",
    "dev_tokens_pad = pad_func(dev_tokens)\n",
    "dev_probablities_pad = pad_func(dev_probablities)\n",
    "dev_feature_pad = pad_func(dev_feature)\n",
    "dev_attention = gen_attention(dev_tokens_pad)\n",
    "\n",
    "dev_tokens_pad, dev_probablities_pad, dev_feature_pad, dev_attention = func_shuffle(dev_tokens_pad, dev_probablities_pad, dev_feature_pad, dev_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xa8Kf3q2rgdx"
   },
   "outputs": [],
   "source": [
    "#defining the model class\n",
    "class ErnieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErnieModel, self).__init__()\n",
    "        self.ernie = pre_trained_model\n",
    "        self.linear = nn.Linear(1028, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, tokens, attention, feature_vect):\n",
    "        pooled_output,_ = self.ernie(tokens, attention)\n",
    "        final_op = torch.cat((pooled_output, feature_vect), dim=-1)\n",
    "        linear_output = self.linear(final_op)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B70NVVPoepQP",
    "outputId": "739a312f-19c5-4a3d-ed60-4219eb639586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch number ---->0\n",
      "Training loss ---->0.28569713388170515\n",
      "Total runtime ----> 3010.004336833954 seconds\n",
      "\n",
      "Validation loss ---->0.2809679090976715\n",
      "Total runtime ----> 88.51337027549744 seconds\n",
      "\n",
      "Validation loss is (inf --> 0.28097).  Saving model ...\n",
      "Running epoch number ---->1\n",
      "Training loss ---->0.2846809416157859\n",
      "Total runtime ----> 3105.1901240348816 seconds\n",
      "\n",
      "Validation loss ---->0.2809678995609283\n",
      "Total runtime ----> 84.90873312950134 seconds\n",
      "\n",
      "Validation loss is (0.28097 --> 0.28097).  Saving model ...\n",
      "Running epoch number ---->2\n",
      "Training loss ---->0.2851088905334473\n",
      "Total runtime ----> 3011.5923867225647 seconds\n",
      "\n",
      "Validation loss ---->0.2809678995609283\n",
      "Total runtime ----> 83.30080127716064 seconds\n",
      "\n",
      "Validation loss is (0.28097 --> 0.28097).  Saving model ...\n",
      "Running epoch number ---->3\n",
      "Training loss ---->0.28478698338781083\n",
      "Total runtime ----> 3023.156350374222 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model = ErnieModel()\n",
    "model_path = 'emphasismodel.pth'\n",
    "early_stopping = EarlyStopping(model_path,4,True)\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.0001)\n",
    "loss_func = nn.MSELoss(reduction = 'mean')\n",
    "batch = 32\n",
    "for epoch_num in range(5):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    print(\"Running epoch number ---->{}\".format(epoch_num))\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(0, len(train_tokens_pad), batch):\n",
    "        model.zero_grad()\n",
    "        t_tokens_pad, t_probablities_pad, t_feature_pad, t_attention = func_shuffle(train_tokens_pad[i:i+batch], train_probablities_pad[i:i+batch], train_feature_pad[i:i+batch], train_attention[i:i+batch])\n",
    "        train_probas = model(torch.tensor(t_tokens_pad), torch.tensor(t_attention), torch.tensor(t_feature_pad))\n",
    "        train_grd_truth = []\n",
    "        for i in t_probablities_pad:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            train_grd_truth.append(p)\n",
    "        train_batch_loss = loss_func(train_probas, torch.tensor(train_grd_truth))\n",
    "        training_loss.append(train_batch_loss.item())\n",
    "        train_batch_loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Training loss ---->{}\".format((np.average(training_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    \n",
    "    #Validation Run\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(dev_tokens_pad), batch):\n",
    "        d_tokens_pad, d_probablities_pad, d_feature_pad, d_attention = func_shuffle(dev_tokens_pad[i:i+batch], dev_probablities_pad[i:i+batch], dev_feature_pad[i:i+batch], dev_attention[i:i+batch])\n",
    "        dev_probas = model(torch.tensor(d_tokens_pad), torch.tensor(d_attention), torch.tensor(d_feature_pad))\n",
    "        dev_grd_truth = []\n",
    "        for i in d_probablities_pad:\n",
    "            p = []\n",
    "            for j in i:\n",
    "                q=[]\n",
    "                q.append(j)\n",
    "                p.append(q)\n",
    "            dev_grd_truth.append(p)\n",
    "        dev_batch_loss = loss_func(dev_probas, torch.tensor(dev_grd_truth))\n",
    "        validation_loss.append(dev_batch_loss.item())\n",
    "        \n",
    "    print(\"Validation loss ---->{}\".format((np.average(validation_loss))))\n",
    "    print(\"Total runtime ----> %s seconds\\n\" % (time.time() - start_time))\n",
    "    early_stopping(np.average(validation_loss), model)\n",
    "\n",
    "    if early_stopping.early_stop is True:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "torch.save(model,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the trained model\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "tokenized_test_text = []\n",
    "for i in testWords:\n",
    "    sent = \"\"\n",
    "    for j in i:\n",
    "        if sent == \"\":\n",
    "            sent += j\n",
    "        else:\n",
    "            sent = sent + \" \" + j\n",
    "    tokenized_test_text.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on Test Dataset\n",
    "test_prob=[]\n",
    "\n",
    "for batch_data in tokenized_test_text:\n",
    "    tokens = tokenizer.tokenize(batch_data)\n",
    "    tid = tokenizer.encode_plus(tokens, add_special_tokens=False, return_attention_mask=False, return_tensors='pt')\n",
    "    test_probas = model(tid['input_ids'])\n",
    "    test_probas=test_probas.data\n",
    "    out = batch_data.split(\" \")\n",
    "    temp_ans = []\n",
    "    index = 0\n",
    "    for i in out:\n",
    "        if (len(tokenizer.tokenize(i))) == 1:\n",
    "            temp_ans.append(test_probas[0][index].item())\n",
    "            index = index + 1\n",
    "        else:\n",
    "            holder = []\n",
    "            for j in range(len(tokenizer.tokenize(i))):\n",
    "                holder.append(test_probas[0][index].item())\n",
    "                index = index + 1\n",
    "            prb = np.average(holder)\n",
    "            temp_ans.append(prb) \n",
    "    test_prob.append(temp_ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = We 'll be closed from 12/24 to 1/1 . See you in the New Year !\n",
      "Emphasis Values = [0.44879353046417236, 0.466549813747406, 0.5147578120231628, 0.44036665558815, 0.489033579826355, 0.4737546344598134, 0.48464372754096985, 0.4211202661196391, 0.4757114350795746, 0.4624374508857727, 0.43629321455955505, 0.523070216178894, 0.5038394331932068, 0.470909059047699, 0.5153239965438843, 0.45707380771636963]\n",
      "\n",
      "Sentence = No matter how hard you work , someone else is working harder .\n",
      "Emphasis Values = [0.38491129875183105, 0.3850019574165344, 0.5098055005073547, 0.4513833224773407, 0.40719500184059143, 0.48779892921447754, 0.45820364356040955, 0.39396074414253235, 0.47681671380996704, 0.3848966956138611, 0.4817645251750946, 0.5142143964767456, 0.38491830229759216]\n",
      "\n",
      "Sentence = The less I needed , the better I felt .\n",
      "Emphasis Values = [0.46332982182502747, 0.4040818512439728, 0.44980910420417786, 0.5157061219215393, 0.49623650312423706, 0.4885362386703491, 0.45792195200920105, 0.45895540714263916, 0.4908824563026428, 0.3445151746273041]\n",
      "\n",
      "Sentence = When someone shows you who they are , believe them the first time .\n",
      "Emphasis Values = [0.47215065360069275, 0.4739573001861572, 0.47419601678848267, 0.4481554627418518, 0.4793816804885864, 0.46626514196395874, 0.48070454597473145, 0.49034634232521057, 0.47389957308769226, 0.48066478967666626, 0.49137601256370544, 0.44101884961128235, 0.4570077657699585, 0.4739099442958832]\n",
      "\n",
      "Sentence = You are not a drop in the ocean . You are the entire ocean in a drop .\n",
      "Emphasis Values = [0.46292978525161743, 0.35762983560562134, 0.456794798374176, 0.4555600583553314, 0.356058806180954, 0.4525981545448303, 0.45641812682151794, 0.4894169867038727, 0.42811694741249084, 0.3556978702545166, 0.4766978323459625, 0.4910675287246704, 0.532541036605835, 0.4920176565647125, 0.46979811787605286, 0.4712674617767334, 0.4513269364833832, 0.44210582971572876]\n",
      "\n",
      "Sentence = All successes begin with self-discipline . It starts with you .\n",
      "Emphasis Values = [0.46067237854003906, 0.46133604645729065, 0.46118924021720886, 0.41859397292137146, 0.4727667570114136, 0.4602195620536804, 0.4608360528945923, 0.4610828757286072, 0.4017989933490753, 0.460857629776001, 0.4611198902130127]\n",
      "\n",
      "Sentence = It 's not a question of learning much . On the contrary . It 's a question of unlearning much .\n",
      "Emphasis Values = [0.45685118436813354, 0.3917142301797867, 0.45592376589775085, 0.40579137206077576, 0.3715882897377014, 0.41583240032196045, 0.5330738425254822, 0.4078839421272278, 0.4193369746208191, 0.4442017078399658, 0.45613202452659607, 0.5300062894821167, 0.4198096692562103, 0.4371168911457062, 0.4138134717941284, 0.4539485573768616, 0.3929769992828369, 0.4189320504665375, 0.4926458150148392, 0.4409903287887573, 0.44906309247016907]\n",
      "\n",
      "Sentence = When I count my blessings , I count you twice .\n",
      "Emphasis Values = [0.4545161724090576, 0.3692556619644165, 0.36320480704307556, 0.46227380633354187, 0.4854559004306793, 0.44907572865486145, 0.41114187240600586, 0.3614553213119507, 0.37061381340026855, 0.3767818808555603, 0.3750966787338257]\n",
      "\n",
      "Sentence = Feel the fear and do it anyway .\n",
      "Emphasis Values = [0.4355027675628662, 0.435762882232666, 0.4357627332210541, 0.4366839528083801, 0.43558603525161743, 0.4599856734275818, 0.434684693813324, 0.43572622537612915]\n",
      "\n",
      "Sentence = A warm smile is the universal language of kindness .\n",
      "Emphasis Values = [0.4716328978538513, 0.4603457450866699, 0.4766574203968048, 0.47401198744773865, 0.5024442076683044, 0.48568761348724365, 0.44541531801223755, 0.49911755323410034, 0.4817091226577759, 0.4741515815258026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predicted Probablities\n",
    "for i in range(10):\n",
    "    print(\"Sentence = {}\".format(tokenized_test_text[i]))\n",
    "    print(\"Emphasis Values = {}\\n\".format(test_prob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EmphasisSelection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
